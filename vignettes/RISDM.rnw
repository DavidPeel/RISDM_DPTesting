\documentclass{article}

%\VignetteIndexEntry{modelDescription}

%\usepackage{times}
\usepackage{mathptmx}
\usepackage{nicefrac}
\usepackage[latin1]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{color}
\usepackage{bm}
\usepackage{url}
\usepackage{authblk}
\usepackage{lineno}
% \usepackage{xr}
\usepackage{endfloat}

% \externaldocument[SI-]{IgnoreInclProbs12_SI}

\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
	\expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
	\expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
	\renewenvironment{#1}%
	{\linenomath\csname old#1\endcsname}%
	{\csname oldend#1\endcsname\endlinenomath}}%
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
	\patchAmsMathEnvironmentForLineno{#1}%
	\patchAmsMathEnvironmentForLineno{#1*}}%
\AtBeginDocument{%
	\patchBothAmsMathEnvironmentsForLineno{equation}%
	\patchBothAmsMathEnvironmentsForLineno{align}%
	\patchBothAmsMathEnvironmentsForLineno{flalign}%
	\patchBothAmsMathEnvironmentsForLineno{alignat}%
	\patchBothAmsMathEnvironmentsForLineno{gather}%
	\patchBothAmsMathEnvironmentsForLineno{multline}%
}

%operators
\newcommand{\ev}[1]{{\rm E}\left(#1\right)}
\newcommand{\sd}[1]{{\rm SD}\left(#1\right)}
\newcommand{\pr}[1]{{\rm Pr}\left(#1\right)}
\newcommand{\lo}[1]{{\rm log}\left(#1\right)}
\newcommand{\ex}[1]{{\rm exp}\left(#1\right)}
%vectors
\newcommand{\vs}{\bm{s}}
\newcommand{\vy}{\bm{y}}
\newcommand{\vp}{\bm{p}}
\newcommand{\vth}{\bm{\theta}}
\newcommand{\vb}{\bm{\beta}}
\newcommand{\vg}{\bm{\gamma}}
\newcommand{\vx}{\bm{x}}
\newcommand{\vw}{\bm{w}}



\linespread{1.25}
\linenumbers
\raggedright
\setlength{\parindent}{15 pt} % Default is 15pt.

\author[1]{CSIRO team for ABARES project (H\&B and D61)}

%\affil[1]{Data61 CSIRO, Hobart, Tasmania, Australia}

\title{RISDM package vignette}
\date{\today}

\begin{document}
\pagenumbering{arabic}
\maketitle

<<preliminaries, echo=FALSE>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
@

\section*{Background}

Data taxonomy ...
\begin{enumerate}
	\item Presence-only
	\item Presence-absence
	\item Abundance-only (?)
	\item Abundance-absence
\end{enumerate}

Spatial scope ... national, or maybe down to level of state or local govt depending on species perhaps ...

Temporal scope ...

Taxonomic scope ...


\section{Model Formulation}

THIS SECTION NEEDS UPDATING

We have been talking about basing the data integration problem on the simple Poisson point process \citep[e.g.][]{cre93}. This is not a unique approach, as it has been done in \citet{fit15} and \citet{fle19}. However, those previous works do not describe the breadth of data types that we have hear, nor do they consider space and time as explicit terms. Hopefully both these issues are surmountable.

Here, a model will be sketched that allows for these different data sources and for space-time effects.  It will not be complete, but it is a starting point.  It is also not an estimation strategy -- details of that will have to be worked out later. Unfortunately, due to haste, many of the variables are assumed to be `obvious'. Hopefully they are!\footnote{Apologies if not}

Let's start with a very general process model, based on a Poisson point process. This is the distribution model of the pest species. Let the country/study area (call it $\mathscr{A}$) be gridded so that there are $N$ cells, all of area $A_i$ for cell $i=1\ldots N$. Also let the spatial domain be referenced by the duple $\vs$ and use the shorthand $\vs_i$ to indicate the location of the $i^{th}$ cell. Then the distribution of the pest species can be governed by the intensity for cell $i$ at time $t$ as\footnote{Notation has already gone awry}
\begin{align*}
\Lambda_{it}&=\int_{\vs\in \mathscr{A}_i}\tilde{\lambda}_{t}(\vs)d\vs\\
&= \sum_{i\in\mathscr{A}_i}\lambda_{t}(\vs_i)A_i\,,
\end{align*}
where the quantity $\lambda_t(\vs_i)$ in the second line is the average intensity in the cell with centroid $\vs_i$. The total intensity for the entire study region $\mathscr{A}$ at time $t$ is
\begin{align*}
\Lambda_t&=\int_{\vs\in\mathscr{A}}\tilde{\lambda}_{t}(\vs)d\vs\\
	&\sum_{i=1}^N\Lambda_{it} = \sum_{i=1}^N\lambda_{t}(\vs_i)A_i\,
\end{align*}
where $N$ is the number of cells in the study region $\mathscr{A}$.

This derivation will centre around $\lambda_{t}(\vs_i)$, and focus on the `data integration' part of the problem.  Linking $\lambda_{t}(\vs_i)$ to covariates and space-time will/should be done by specifying it as a function of covariates and random effects \citep[ala GLM-like models:][]{mcc89,dig07,cre11}. This will look something like:
\begin{align*}
	\lo{\lambda_t(\vs_i)}&=\vx_{it}^\top\vb + u(\vs_i)
\end{align*}
where $\vx_i$ contains all the environmental (possibly time-varying) covariates, and $u(\vs_i)$ is the spatio-temporal random effect and will be assumed to be multivariate Normal. The random effect may be separable, and probably will be for ease of computation -- this assumes that there is some structure to the spatio-temporal process.

\subsection{Relating Point Process to Multiple Data Types}

The real trick in data integration is to try to figure out how all the different data types relate to the point process defining the distribution of the species.  This is not necessarily the distribution that is observed of the species (as there may be a `thinning' in the observation that induces bias). We'll start with the easier-to-relate data types and move on. Spatial and temporal process $u(\vs_i)$ is omitted for now to be added later.

\subsubsection{Presence-Only (PO) Data}

The PO data is superficially the simplest type of data to incorporate.  Following \citet{war10} it can be modelled using $\lambda_t(\vs)$, the intensity of the underlying point process. Here, we can possibly do slightly better and introduce a thinning variable \citep[like][have done for multiple species]{fit15}:
\begin{align*}
	\lo{\lambda_t(\vs_i)^*}&=\lo{b_t(\vs_i)\lambda_t(\vs_i)}=\lo{b_t(\vs_i)} + \lo{\lambda_t(\vs_i)}\\
	&=\vw_{it}^\top\vg + \vx_{it}^\top\vb
\end{align*}
where the non-standard log-link has been used for the thinning process rather than the (canonical) logit. The reason for this is simple: it maintains the convenient additive structure on the link scale. In reality, and as \citet{fit15} point out\footnote{in a footnote}, there is likely to be little difference between the log and the logit \textit{just as long as $\pi_t(\vs_i)$ is close to zero}. This is likely to be the case in most real data sets as only a small fraction of individuals are observed. Note though that there is no reason (apart form convenience) to not use an alternative link function (e.g. logit / probit / cloglog).

It should be noted that the approach of \citet{fit15} is a \textit{multispecies} model. There the thinning/bias term is common to all species, but not the distribution term. The approach \textit{should} work for single species, but may have reduced effectiveness.  Actually, now that I think about it...  In a single species setting, and considering PO data only, the approach reduces to that of \citet{war13} which will suffer when the thinning process is dependant upon the same variables as the distribution. Fortunately, we also have PA and other types of data, which will speak to the distribution model (but not the thinning).

\subsubsection{Presence-Absence (PA) Data}

To set notation, assume that the PA data is notated as $y_{pa}(it)$ and is the result of surveying the finite area $a_{pa}(it)$. Under the point process, the probability of observing an individual at any space-time location ($\vs_{it}$) is one minus the probability of observing no individuals at that location, viz \begin{align*}
	\pi_{it}&=\ev{y_{pa}(it)}=1-\ex{-\lo{\alpha_{pa}}\lambda_{it}a_{pa}(it)}\\
	&=1-\ex{-\ex{\alpha_{pa}+\vx_{it}\vb + \lo{a_{pa}(it)}}}.
\end{align*}
Thus, the presence absence data is, once again, linked to the point process and the data speak directly to the parameters. This is the same trick as used by \citet{fit15}, and earlier researchers \citep[e.g.][]{XXX}. This model is just the same as a GLM with a complementary log-log link function. This formulation is nice though as, unlike the logit or probit, the search effort for each observation ($a_{pa}(it)$) enters quite naturally.

It is probable that the extra parameter $a_{pa}$ is needed to account for the \textit{catchability} (detection probability) of the PA collection method. Like the PO data, we shouldn't necessarily expect that an individual will be observed even if it is there \citep[e.g.][]{rid98,mac05,mar05}. However, it is \textit{probably reasonable} to assume that the catchability is constant for all PA data.  Isn't it? GH: I don't think so but we can discuss.

It may be important to note that this model assumes that the data are superficially what we think that they are. In particular, there is no preference given to certain geographical locations or habitats.  If that is the case, then more complicated approaches will have to be employed \citep[see][]{dig10,pat11}.

\subsubsection{Abundance Data}

The model for PA data gives us some clues about how to proceed with describing abundance data too. Let $y_a(it)$ be the observed abundance at space-time index $i$ and $t$ measured in area $a_a(it)$. Then, according to the Poisson point process assumption, we know that the the expected number of presences (abundance) within the area $\mathscr{A}_{a(it)}$, which has area $a_a(it)$, is
\begin{align*}
	\ev{y_a(it)}&=\int_{\vs_i\in\mathscr{A}_{a(it)}}\lo{\alpha_a}\lambda_{t}(\vs)d\vs\\
	&=\lo{\alpha_a}\lambda_t(\vs_i)a_a(it),
\end{align*}
where the second equality hold only if the area $\mathscr{A}_{a(it)}\subseteq\mathscr{A}_i$ (the abundance sample was taken wholly within one, and only one, cell). The $\alpha_a$ term, like $\alpha_{pa}$ before, is likely to be required to account for catchability of the sampling method to measure abundance. It might be that we can usefully assume that $\alpha_a=\alpha_{pa}$ but this may also be a bad idea.

Note also that with the Poisson point process assumption, the variation around $\ev{y_a(it)}$ will also be Poisson. If the data do hold this assumption, then...  Well, then there may be trouble.  It (the Poisson process) assumption is likely to be a useful assumption, but it is also likely to be one that could be shown to be inappropriate.  It is also the state of the art in data integration.  Fortunately though, with the space-time random effect, we can legitimately say that this isn't a simple Poisson process.  Rather it is a log-Gaussian Cox process \citep{XXX}.

\subsubsection{Ordinal Data}

This is where this description gets pretty uncertain pretty quickly.  Sorry.  It is the most challenging and least straight-forward part of the observation model(s). The suggestion below is based on the ideas from ordinal regression \citep{mcc80}, and contains similar ideas to those introduced previously. Everything will need to be checked, carefully.

Suppose that the categories are related to the covariates through the same intensity function as before ($\lambda_{it}=\ex{\vx_{it}^\top\vb+u(\vs_{it})+\lo{A_{i}}}$). And suppose that the probability of any ordinal datum, $y_o(it)$ say, of being below the $k^{th}$ category is
\begin{align*}
	\pr{y_o(it)\leq k}&=\ex{-\ex{\theta_k+\lo{\lambda_{it}}}}\\
	&=\ex{-\ex{\theta_k+\vx_{it}^\top\vb+u(\vs_{it})+\lo{A_{i}}}}
\end{align*}
This isn't entirely made up and does have some good reasoning\footnote{that still needs to be described well and explored}. Check out the Wikipedia page on ordinal regression.  Also note that this approach is called `the proportional hazards model' by \citet{mcc80} as it has the same function form as the more famous model for survival data \citep[][]{cox72} but its genesis and interpretation is slightly different.

The thing that is really worrisome is that the difference between the categories is assumed to be the same scale as the changes in the intensity function. Currently, it is not obvious how to link the point process intensity \textit{theoretically} to this proportional hazards.  Perhaps there is a link between the point process intensity and the hazard function from survival analysis?  In any case, it may be possible to re-scale the intensity, so that the model is
\begin{align*}
	\pr{y_o(it)\leq k}&=\ex{-\ex{\theta_k+\lo{\lambda_{it}\gamma}}}
\end{align*}
which treats the intensity as a covariate in the ordinal model. Note that this model is still identifiable, I think.  It has only one extra parameter for all the different ordered categories.

\section{Computation}

\textbf{???}

\begin{itemize}
	\item How to calculate likelihood for PO data?  Probably through grid-based methods, but could also go through the quadrature thing \citep[e.g.][]{ber92,war10}.
	\item Weighting likelihoods (ad-hoc and arbitrary).
	\item Inference approaches
	\item Likely engines (INLA, STAN, MCMC, Laplace approx, ...)
\end{itemize}

\subsection{Let's talk about mesh}

To guide mesh construction, consideration of previous recommendations is required. It will be seen though that there is a fair amount of discretion available for mesh construction.
The key function in R package INLA is inla.mesh.2d(); note that an earlier function inla.mesh.create.helper(), as discussed for example by \citet{Cameletti2013} for geostatistical data analysis, is now obsolete. In choosing arguments for inla.mesh.2d(), \citet[][Ch. 4]{Krainski2018} comment that ``some care'' is needed when building the mesh for point processes, and then go on to say that the location points are ``usually'' not used for mesh building. Considering log Gaussian Cox processes, however, \citet{Simpson2016} note that although a regular structure is needed to cover the spatial area of interest, nevertheless it is reasonable to allocate greater computational effort in regions that contain greater numbers of observations. This study is however mainly theoretical and does not provide practical guidance for applied examples, with the reader being referred on to \citet{Blangiardo2015} for the specifics of mesh creation. \citet[][Ch. 6]{Blangiardo2015} does provide an earnest attempt to guide reasonable mesh development through R-INLA while including guidance provided by other key sources such as \citet{Lindgren2015}. The latter reference also details the arguments to inla.mesh.2d() and identifies the Neumann boundary condition employed by R-INLA. These latter two references together provide the best overview of mesh selection, but importantly both also indicate that mesh specification remains flexible and may require adjustment for specific applications.

Given the above literature summary, it is perhaps difficult to discern general procedures for mesh construction. For example, in a recent simplifying wrapper for R-INLA that aims to increase accessibility of spatial point processes and other models \citep{Bachl2019}, the problem of mesh construction is unaddressed. Thinking generally, the rIAS package will have to balance the following considerations:
\begin{itemize}
\item computational cost (e.g., number of nodes)
\item specificity to spatial region of interest (e.g., national vs state vs local government areas)
\item ``resolution'' of mesh
\item spatial range of Matern process
\end{itemize}
Note that the last two items will be closely linked to the life history of the study organism: a species with high dispersal ability may have greater ``smoothing'' compared to a slowly dispersing species that persists in clumped patches etc. The spatial range is an important consideration for the overall smoothness of predictions and also the size of the buffer around the domain of interest so as to avoid boundary effects.

Currently, the most up-to-date summary on the details of mesh specification for general R-INLA models is found in \citet[][Ch. 2]{Krainski2018}. A general workflow is a follows. Anticpating non-regular boundaries, it would be recommended to allow for non-convex domain using the function  inla.nonconvex.hull() before applying inla.mesh.2d(); the resulting number of nodes in a mesh object is stored in the mesh object. The R package INLA does provide a shiny app for viewing and critiquing any mesh via the function meshbuilder().

Strategy going foward ... suggest default values to inla.nonconvex.hull and inla.mesh.2d ... however, it will be difficult to automate mesh ``checks'' for say remote unfacilitated application by ABARES etc ... discuss ...

\bibliographystyle{chicagoa}
\bibliography{RISDM_references}

\end{document}
